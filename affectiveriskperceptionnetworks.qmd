---
title: "Affective Risk Perception Networks"
author: "Jan-Erik Johanson, Alisa Puustinen & Oliver Saal"
subtitle: "Draft article for Ã…AU Sociology summer seminar 1-2 June 2023"
abstract: |
  This is an early draft version on a research project I, senior researcher Alisa Puustinen
  at Pelastusopisto and professor Jan-Erik Anderson at University of Tampere have been working
  on. Its main point is to argue for a network conception of risk perception, i.e., that
  individuals' perceptions of hazards and risks are constructed through a complex system of
  interactions, as opposed to direct perception and evaluation of singular risk events.
  
  We recently decided to divide the paper into two: one empirical section for a journal,
  and one theoretical discussion for an invited book chapter.
  
  We especially welcome comments and discussion regarding the theoretical background.
  Ideas, notes, suggestions for reading, etc., welcome!
  
  I did not have time to fix figure formatting in the PDF version, so please see the online version for better quality images:
    
  osaal.github.io/affectiveriskperceptionnetworks
date: last-modified
date-format: long
---

```{r article-setup, include=FALSE}
# Clear environment and save parameters to avoid clashes with previous work
rm(list=ls())
resetpar <- par(no.readonly = TRUE)
# Setup article work directories and libraries
workdir <- "C:/Users/olive/RPROJS/papers/affectiveriskperceptionnetworks"
packages <- c("tidyverse", "naniar", "knitr", "ggstatsplot", "bootnet", "ppcor", "smacof", "qgraph", "NetworkComparisonTest")
lapply(packages, library, character.only=TRUE)
setwd(workdir)

set.seed(12345)
```

# Introduction {#sec-introduction}

# Previous Research and Theory {#sec-previous}

## A Preliminary Definition of Risk, Risk Perception, and Resilience {#sec-definition}

The usage of the three key words (risk, risk perception, and resilience) in the research literature is varied. Some attempts have been made at collating these divergent definitions, notably by Aven [-@aven2022]. They present six distinct perspectives, roughly named the disjunct, broader disjunct, event, consequence, undesirable potentials, and combination perspectives. Depending on the perspective, risks are defined variably as reduction in functionality, events, consequences, and/or uncertainties related to consequences. Similarly, resilience is variably defined as recovering ability, reaction ability, or even not defined at all [@aven2022].

The problems with most of these perspectives, according to Aven [-@aven2022], is that they distinctly draw lines between risks and resilience. Particularly in the two disjunct perspectives as well as the event perspective, the risk ends once the event is declared over and the recovery phase as begun - the two stages are incommensurable.

## Risk and Systems {#sec-risksystems}

Perceptions of flood risk in particular have been found to correlate with perceptions of other technological risks [@zhai2008].

It is important to distinguish between the ontology and the perception of risks. The former refers to the contents, dynamics, and structure of the risk itself, while the latter refers to individual's meaning-makings of risks.

In the space of risk ontology, research has suggested that risks can be divided into global systemic risks (GSR) and conventional risks [@renn2019; @renn2021; @renn2022]. Summarily, GSR's transgress traditional social and scientific boundaries, are intertwined and interconnected with one another, present non-linear causality in their structure, and have non-deterministic effects [@haas2022].

Non-deterministic effects cannot be adequately reduced to deterministic models.

However, risk ontology is not the same as risk perception.

## Risk Perception {#sec-perception}

Risk perception has been widely studied in the past two decades, but is still lacking a coherent theoretical framework [@kellens2013]. Generally speaking, perception has been found to be related to both hazard experiences and reactive behaviour.

Perception is related to behaviour in some way, but the causal connection is unclear [@dekoning2017]. It has been shown that post-event risk decision-making is drastically reactive to the event [@slovic2004; @gallagher2014]. The time after event is also very relevant, with post-event risk perception not only falling, but falling at a speed derivative of the time post-event [@atreya2013; @bin2013; @gallagher2014]. One study found that storm risk perception drops below objective risk probabilities given enough time between events, but rises rapidly upon the following event [@magliocca2018].

Lately, attempts at modelling risk perception as a complex system have been made with fruitful results. Particularly agent-based modelling has been applied to individual risk perception in coal mining [@nai-we2015] and water management [@hyun2019]. Other modelling efforts include system dynamics modelling of risk behaviour, including perceptions, in the COVID-19 pandemic [@korzebor2023] and ecosystem modelling of risk perception in fisheries [@vanputten2011].

Traditional psychometric efforts have been made in risk perception studies, but to our knowledge, no studies have applied the network paradigm. A study on risk perceptions among mineworkers found a two-factor structure termed "Dreaded" and "Unknown" [@alrawad2022].

## Experiences and Perception {#sec-experiences}

The previous literature generally distinguishes between two types of experience: **direct** and **near-miss** experiences. Direct experiences are, as the name implies, directly and non-mediately experienced, while near-miss experiences are situations that could have resulted in the expression of the hazard event, but did not. As an example, a direct experience of a fire could be a stove fire in the individual's kitchen, while a near-miss experience of a fire could be a situation where the individual noticed that they had left flammable materials on an active stove-top, but managed to remove the materials prior to them catching fire.

However, we propose that risk experiences are much more varied than this. We suggest five distinct forms, borrowed from and inspired by inter-group contact theory [ICT, @zhou2020].

1.  Direct non-intensive experience
2.  Direct intensive experience
3.  Extended experience
4.  Imagined experience
5.  Mediated experience

The first two experiences represent the regular direct experience, with a clear distinction: the intensity in the experience is much greater in type (2) than in type (1). Burning food on the stove-top is an example of a direct non-intensive experience, while surviving a house fire is a direct intensive experience.[^1]

[^1]: This distinction already suggests that the underlying function is a linear spectrum from low to high intensity. This thought needs to be further developed.

If extended contact in ICT is "knowing that another ingroup member has at least one cross-group friend" [@zhou2020 p. 9], then extended risk experiences is knowing that a significant other has experienced the hazard event in question.

Imagined experience represents near misses, but with an emphasis on the imaginative part. In this perspective, the causally empowered aspect is not how the experience is 'almost like a risk' (i.e., risk resemblance), but the near-miss as a trigger for considering a potential risk realisation. In other words: the near-miss triggers the individual to consider what the risk could have looked like and how it could have played out, which itself triggers a reconsideration of the probabilities attached to the risk event.

Mediated experience is the experiencing the risk as portrayed in media [cf. @zhou2020 p. 15]. This differs from the extended experience in that the originator of the message is not a significant other of the individual. Risks are experienced mediated all the time: from COVID-19 discussions on social media to news events of the war in Ukraine, individuals are constantly exposed to mediated experiences. In risk perception research, this topic has been studied under the sub-topic of risk communication studies.

# Hypotheses {#sec-hypotheses}

We test four hypotheses in total. These are divided into two groups: the system level and the node level hypotheses:

-   System level:

    -   $H_1^S:$ Experiencing a fire leads to a sparser risk perception network
    -   $H_2^S:$ Experiencing a near-miss leads to a denser risk perception network

-   Node level:

    -   $H_1^N:$ Experiencing a fire leads to a decrease in the centrality of fire in the risk perception network
    -   $H_2^N:$ Experiencing a near-miss leads to an increase in the centrality of fire in the risk perception network

The system level test cannot distinguish between different objects of perception, as we are testing a whole network composed of multiple modes. Thus, the system level test judges a more general causality: that the experience (whether near-miss or direct) causally affects the system of risk perception.

The system- and node-level hypotheses are independent. Because of the complex causal mechanisms of networks, the cumulative effects on the system level of any given input need not match up with a singular node-level effect. Just because the risk perception network changes does not automatically mean that a particular node changes, and vice versa. Whether a node affects system structure or system changes affect singular nodes is dependent on the relations between nodes within the system. As an example of this: networks have two structural properties that researchers can compare using psychometric network tools, global strength and network variance. Global strength simply refers to the sum of the absolute node connection weights in the network, while network variance refers to changes in *any* connection weight. As van Borkulo and colleagues [-@vanborkulo2022] show, either measure can vary independently of the other given certain change configurations between compared networks.

In a similar vein, we suggest that the risk perception network and its constituent nodes may vary independently, giving rise to potential difference in structural and constituent effects. Thus, we divide the hypotheses into two categories.

The direct experience hypotheses $H_1^S$ and $H_1^N$ test the second form of perception-modifying experiences (see @sec-experiences), direct intensive experiences. Based on previous research, we hypothesise that direct intensive experiences lead to a weakening of the affective risk system in favour of the analytic risk system. Thus, evaluations of the probability of risks in general, and of the specific risk that was experienced (e.g., a fire), should become more independent from other risks.

The near-miss hypotheses $H_2^S$ and $H_2^N$, correspondingly, test the fourth form of perception-modifying experiences, imagined experiences. Here, we hypothesise that experiencing a near-miss leads to a strengthening of the affective system, increasing the connections between risk probability assessments in general, and between the risk that was nearly experienced and other risks in specific.

At this point, we do not test the other three forms of risk experiences.

# Materials and Methods {#sec-materialsandmethods}

## Materials {#sec-materials}

### Data Collection {#sec-datacollection}

```{r data-import}
# Import raw data
rawdata <- tibble(read.csv2("emergencyservicesattitudes2023.csv"))
```

We used the Finnish Emergency Services Attitudes 2023 material, consisting of N = 3055 survey respondents. The survey was collected between 17 January and 2 February 2023 as an on-line panel study, with sampling corresponding to central geographical, gender, and age distributions of the mainland Finnish population aged 18-79 years. Participation was voluntary and informed consent was retrieved.

### Dependent Variables {#sec-dependentvariables}

In the survey, respondents were asked to gauge the probability that they would encounter any of fourteen listed risks or threats in their personal lives. The risks are listed below, with short-hand names in brackets.

-   "Extreme weather phenomena (flood, drought, storm etc.)" \[Weather\]
-   "Traffic accident" \[Traffic\]
-   "Fire" \[Fire\]
-   "Nuclear power plant disaster" \[Nuclear\]
-   "Workplace accident" \[Work\]
-   "Leisure-time accident (at home/at hobbies etc.)" \[Leisure\]
-   "Attack with weapons directed at the Finnish state" \[War\]
-   "Great environmental accident, such as an oil catastrophe" \[Environmental\]
-   "Dangerous transmittable disease, pandemic" \[Pandemic\]
-   "Illness, severely falling ill (e.g., life-threatening cancer)" \[Illness\]
-   "Act of violence" \[Violence\]
-   "Operational failure directed at e.g., electrical, water, or food supply, information traffic or the informational system" \[Operational\]
-   "Influencing through information, fake news, other so-called hybrid threats" \[Hybrid\]
-   "Growth of tensions between social groups, polarization" \[Polarization\]

For each variable, respondents judged their perceived subjective probability on a four-point scale as *very probable*, *somewhat probable*, *somewhat improbable*, *very improbable* and *cannot say*. These variables are later referred to as *risk perception variables*. Excluding the uncertain responses as missing values, the distributions are presented in @fig-dependents.

```{r}
#| label: fig-dependents
#| fig-cap: "Distributions of risk perception variables"
#| fig-subcap: 
#| -  Weather
#| -  Traffic
#| -  Fire
#| -  Nuclear
#| -  Work
#| -  Leisure
#| -  War
#| -  Environmental
#| -  Pandemic
#| -  Illness
#| -  Violence
#| -  Operational
#| -  Hybrid
#| -  Polarization
#| layout-ncol: 3

data <- rawdata %>%
  dplyr::select(question_3_row_1:question_3_row_14)

data <- data %>%
  rename(Weather = question_3_row_1,
         Traffic = question_3_row_2,
         Fire = question_3_row_3,
         Nuclear = question_3_row_4,
         Work = question_3_row_5,
         Leisure = question_3_row_6,
         War = question_3_row_7,
         Environmental = question_3_row_8,
         Pandemic = question_3_row_9,
         Illness = question_3_row_10,
         Violence = question_3_row_11,
         Operational = question_3_row_12,
         Hybrid = question_3_row_13,
         Polarization = question_3_row_14)

data <- replace_with_na_all(
  data,
  condition = ~.x == 5)

ggplot(data) + geom_bar(aes(x = Weather)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Traffic)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Fire)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Nuclear)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Work)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Leisure)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = War)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Environmental)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Pandemic)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Illness)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Violence)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Operational)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Hybrid)) + coord_cartesian(ylim=c(0,3055))
ggplot(data) + geom_bar(aes(x = Polarization)) + coord_cartesian(ylim=c(0,3055))
```

The risks judged to be most likely on average were pandemics, illnesses, and hybrid threats. The least likely experiences were nuclear power plant disasters, wars against Finland, and workplace accidents. Judging from missing values, respondents were most certain about the probability of experiencing extreme weather phenomena, and least certain about workplace accidents (@fig-na-analysis).

```{r}
#| label: fig-na-analysis
#| fig-cap: "Missing values by variable"

vis_miss(data)
```

### Control Variables {#sec-controlvariables}

We used two control variables:

-   Direct experience of fire
-   Near miss-experience of fire

Respondents were asked whether they had had a direct experience of a fire in the past twelve months. They were similarly asked whether they had experienced a near-miss situation, i.e., a scenario that could have ended in a fire but did not. These two variables operationalise two distinct hypotheses (see @sec-hypotheses) regarding the network structure. Their descriptive statistics are presented in @fig-substantive-desc.

```{r}
#| label: fig-substantive-desc
#| fig-cap: "Substantive Control Variables"
#| fig-subcap: 
#|   - "Direct Experience of Fire"
#|   - "Near-Miss Experience of Fire"
#| layout: [[1, 1]]
#| cache: true

data <- rawdata %>%
  dplyr::select(question_39_row_1:question_40) %>%
  rename(Direct = question_39_row_1,
         Near.Miss = question_40) %>%
  bind_cols(data, .)

data$Direct <- case_match(
  data$Direct,
  1 ~ 1,
  2 ~ 0,
  3 ~ NA,
  .default = NA
)
data$Near.Miss <- case_match(
  data$Near.Miss,
  1 ~ 1,
  2 ~ 0,
  3 ~ NA,
  .default = NA
)

subsdata <- tibble(
    "Direct" = factor(
      data$Direct,
      levels = c(0:1),
      labels = c("No", "Yes")
    ),
    "Near.Miss" = factor(
      data$Near.Miss,
      levels = c(0:1),
      labels = c("No", "Yes")
    )
  )

ggplot(subsdata) + geom_bar(aes(x = Direct))
ggplot(subsdata) + geom_bar(aes(x = Near.Miss))
```

## Methods {#sec-methods}

All analyses were completed in RStudio version 2023.03.0 build 386 [@positteam2023]. Data were processed using the `tidyverse` [@wickham2019] and `naniar` [@tierney2023] packages. Plots and graphs were generated in `ggplot2` [@wickham2016], including the `ggstatsplot` [@patil2021] extension package. MDS rotations were done in `smacof` [@mair2022] and partial correlations with `ppcor` [@kim2015]. Network comparison was conducted in `NetworkComparisonTest` [@vanborkulo2022]. Network graphs and related plotting and statistics were conducted in `bootnet` [@epskamp2018b] and `qgraph` [@epskamp2012]. The article was written in RMarkdown and compiled using the `knitr` [@xie2014; @xie2015; @xie2023] package for reproducibility.

We started by fitting a model for the graphical representation of networks. We modelled a partial correlation network of the risk perception variables using the multidimensional scaling (MDS) technique [@jones2018]. We thus retrieved Shepard curves for each MDS type and chose model type by balancing parsimony with increasing fit (reduction of the stress factor, see [@jones2018]). This model was only used to construct the MDS model.

For the main analysis, we fitted a pairwise Markov random field (PMRF) model as a Gaussian graphical model (GGM). We used Spearman correlations and the EBICglasso estimator function [@epskamp2012; @foygel2010]. We fitted two models: a non-parametric model and a case-dropping network model. The former was used for presenting and analysing the network as a graph and as a plot of centrality indices, and the latter was used for analysing the stability of centrality indices on both system- and node levels [@epskamp2018b].

For robustness checks, we use the case-dropping model and retrieve *correlation stability (CS) coefficients*. The case-dropped model can be correlated with the original model to estimate how well the case-dropped model centrality measures represent the original. The CS coefficients then determine the proportion of cases that may be dropped whilst still retaining a correlation of $r = p$, where $p$ is the preferred correlational level, in $(1-\alpha)/100$ % of cases. For this study, we use the default values of $r = 0.7$ and $95$ per cent [@epskamp2018b].

Previous research suggests that, when interpreting differences between centralities (e.g., if one risk perception is more central to the network than another), the centrality stability coefficient should preferably be above 0.5, with higher equalling a more stable centrality measure [@epskamp2018b; @fried2022].

To compare between groups, we estimated group-wise Gaussian graphical models and used the network comparison test (NCT) as outlined by Fried, Epskamp, Veerman and van Borkulo [-@fried2022]. The NCT works by calculating a test statistic of choice, constructing a distribution of the statistic using random rearranging sub-sampling of the network data, and comparing the simulated statistic under a null hypothesis to the observed statistic [@fried2022, pp. 143-144]. We retrieve two levels of tests: *omnibus* tests measuring differences in global strength and network variance, and *specific* tests measuring differences in strength, closeness, betweenness, and expected influence of the "Fire" node in either group.

Parts of this manuscript have been written using the large language model (LLM) Perplexity.AI [@perplexityai2023]. The LLM has been used to support literature review. No data processing or data analysis has been conducted using the LLM.

# Results {#sec-results}

## Network Estimation and Robustness {#sec-networkestimation}

We first started by estimating the network model using the `EBICglasso` algorithm.

```{r}
#| label: model-estimation

# Estimate the network. Change "nCores" to a fitting amount of processor cores.
network <- estimateNetwork(
  dplyr::select(data, Weather:Polarization),
  default = "EBICglasso",
  corMethod = "cor",
  corArgs = list(method = "spearman"),
  nonPositiveDefinite = "continue"
)
```

To graphically plot the network, we modelled the resulting network using the MDS method. Shepard plots (@fig-mds-shepards) suggested that we use the spline model.

```{r}
#| label: fig-mds-shepards
#| fig-cap: "Shepard stress plots of MDS configurations"
#| fig-subcap: 
#| -  "Ordinal"
#| -  "Interval"
#| -  "Ratio"
#| -  "Spline"
#| layout-ncol: 2

dis <- sim2diss(network$graph)
mds_ordinal <- mds(dis, type = "ordinal")
mds_interval <- mds(dis, type = "interval")
mds_ratio <- mds(dis, type = "ratio")
mds_spline <- mds(dis, type = "mspline")

plot(mds_ordinal,
     plot.type="Shepard",
     sub = paste0("Stress = ", round(mds_ordinal$stress, 2)))
plot(mds_interval,
     plot.type="Shepard",
     sub = paste0("Stress = ", round(mds_interval$stress, 2)))
plot(mds_ratio,
     plot.type="Shepard",
     sub = paste0("Stress = ", round(mds_ratio$stress, 2)))
plot(mds_spline,
     plot.type="Shepard",
     sub = paste0("Stress = ", round(mds_spline$stress, 2)))
```

We then retrieved a robust model of the graph using the `bootnet` package [@epskamp2018b]. The model uses the `EBICglasso` algorithm, which searches for the model that locally minimises the extended Bayesian Information Criterion [@epskamp2018b; @foygel2010]. The estimation is done using Spearman correlations because our data are expected non-normally distributed. While polychoric correlations would be possible, the small sample sizes in sub-population comparison may result in a non-positive definite correlation matrix, which disables analysis.^[To test equivalency, we retrieved a bootstrapped network using polychoric correlations with the `cor_auto` method in `bootnet`, and correlated the resulting network with the network retrieved using Spearman correlations. The resulting Pearson correlation was 0.992. Thus, under our conditions, the two methods produce virtually the same results, and we opt for the simpler Spearman correlations due to calculation efficiency.] We retrieved both non-parametric and case-dropping networks for analysis.

```{r}
#| label: bootstraps

# Run the following only once if possible, as they take time to run
# Completed in 4 min 20 s. on AMD Ryzen 9 5900HS with nCores = 16

bootstrap <- bootnet(
  dplyr::select(data, Weather:Polarization),
  nBoots     = 1000,
  default    = "EBICglasso",
  type       = "nonparametric",
  nCores     = 16,               # Change to appropriate processor core count
  statistics = c("edge",
                 "strength",
                 "closeness",
                 "betweenness",
                 "expectedInfluence"
                 ),
  verbose    = TRUE,
  corMethod = "cor",
  corArgs = list(method = "spearman")
)

# Completed in 4 min 10 s. on AMD Ryzen 9 5900HS with nCores = 16
casedrop <- bootnet(
  dplyr::select(data, Weather:Polarization),
  nBoots     = 1000,
  default    = "EBICglasso",
  type       = "case",
  nCores     = 16,               # Change to appropriate processor core count
  statistics = c("edge",
                 "strength",
                 "closeness",
                 "betweenness",
                 "expectedInfluence"
                 ),
  verbose    = TRUE,
  corMethod = "cor",
  corArgs = list(method = "spearman")
)

stability <- corStability(casedrop, verbose = FALSE)
```

The robust network is plotted using the two-dimensional MDS solution as its layout, enabling us to interpret graphical distance as a proxy for vertex strength. @fig-bootstrapped-model-1 shows the network graph.

```{r}
#| label: fig-bootstrapped-model-1
#| fig-cap: "Bootstrapped network. Edge labels indicate partial correlation strength."

# Plot network graph with previously configured layout
plot(
  bootstrap$sample,
  layout = mds_spline$conf,
  edge.labels = TRUE,
  theme = "colorblind",
  maximum = 0.5
)
```

```{r}
#| label: tbl-network-stats
#| tbl-cap: "Descriptive statistics of network nodes"
summary <- tibble(
  Node = rownames(network$graph),
  Max = summarise(as_tibble(network$graph), across(everything(), max)) %>% unlist(., use.names=FALSE),
  Min = summarise(as_tibble(network$graph), across(everything(), min)) %>% unlist(., use.names=FALSE),
  Mean = summarise(as_tibble(network$graph), across(everything(), mean)) %>% unlist(., use.names=FALSE),
  SD = summarise(as_tibble(network$graph), across(everything(), sd)) %>% unlist(., use.names=FALSE)
)

cent <- centralityTable(network$graph, standardized = FALSE) %>%
  reshape2::dcast(
    node ~ measure,
    value.var = "value"
  ) %>%
  rename(
    Node = "node"
  )

summary <- left_join(summary, cent, by = "Node")

kable(
  summary,
  digits = 2,
  col.names = c("Node", "Maximum", "Minimum", "Mean", "SD", "Betweenness", "Closeness", "Strength", "Expected Influence")
  )
```

There are two distinct connections in the network: Traffic-Fire (`r round(network$graph[2,3], 2)`) and Hybrid-Polarization (`r round(network$graph[13,14], 2)`). Most other connections are fairly weak, with low-to-medium partial correlations.

The strongest connections are between Traffic and Fire (strength = `r round(as.numeric(summary[2, "Max"]), 2)`). Most nodes have only positive connections, with nuclear, work, illness, and hybrid being the only nodes with connections below zero. Mean connection strength is low across most nodes, but variation among nodes is comparatively high.

Four node connections are negative: Fire-Hybrid (`r round(network$graph["Fire","Hybrid"], 2)`), Nuclear-Hybrid (`r round(network$graph["Nuclear","Hybrid"], 2)`), Work-Illness (`r round(network$graph["Work","Illness"], 2)`), and Hybrid-Work (`r round(network$graph["Hybrid","Work"], 2)`).

@fig-bootstrapped-model-2 shows node centrality measures by node and measure. Strength varies between `r round(as.numeric(summary[1, "Strength"]), 1)` for extreme weather phenomena and `r round(as.numeric(summary[3, "Strength"]), 1)` for fire.

Closeness is very low, with every node around 0.005 in closeness. The similarity in scores suggests that nodes are fairly evenly connected.

Betweenness, however, is much more varied. Weather presents a betweenness of zero, meaning that it does not feature in any shortest path connection in the network. At the other end, Environmental and Illness feature in fourteen shortest paths, thus being very central nodes in the network.

As there are very few negative connections, expected influence mirrors strength for most nodes. Fire, Nuclear, Work, Illness, Operational, and Hybrid present at least one negative connection, which results in their expected influence being slightly lower than their strength.

@tbl-sml shows the smallworldness statistics for the network. The smallworldness index is not substantially larger than either 1 or 3, suggesting that the network is not a small-world network [@humphries2008; @newman2003]. Neither transitivity nor average path length (APL) is substantially higher than their randomly generated counterparts, further suggesting that the network is not considered a small-world network.

```{r}
#| label: tbl-sml
#| tbl-cap: "Smallworldness statistics of bootstrapped network, 1000 permutations for random networks"

sml <- as_tibble(as.list(smallworldness(network$graph)))
sml <- round(sml, digits = 3)

sml <- unite(
  sml,
  col="transCI",
  c("trans_rnd_lo", "trans_rnd_up"),
  sep="-"
)
sml <- unite(
  sml,
  col="averagelengthCI",
  c("averagelength_rnd_lo", "averagelength_rnd_up"),
  sep="-"
)
sml$transCI <- paste0("(",sml$transCI,")")
sml$averagelengthCI <- paste0("(",sml$averagelengthCI,")")

sml <- unite(
  sml,
  col="trans_rnd",
  c("trans_rnd_M", "transCI"),
  sep=" "
)
sml <- unite(
  sml,
  col="averagelength_rnd",
  c("averagelength_rnd_M", "averagelengthCI"),
  sep=" "
)

kable(
  sml,
  digits = 3,
  col.names = c("Smallworldness Index", "Transitivity", "APL", "Random transitivity (95% CI)", "Random APL (95% CI)")
)
```

```{r}
#| label: fig-bootstrapped-model-2
#| fig-cap: "Centrality measures of bootstrapped network, raw scores."

# Plot centrality data
centralityPlot(
  bootstrap$sample,
  include = "All"
  )
```

@fig-bootstrapped-model-3 shows the stability of the centrality measures when case-dropping. The CS coefficients are listed in @tbl-cscoef. Strength, closeness and expected influence are all stable, with up to 75 per cent of cases having to be dropped before the correlation drops below 0.7 in 95% of bootstraps. However, betweenness does not seem to be stable enough to interpret, at a CS coefficient of only `r round(stability[["betweenness"]], 2)`.

| Measure            | CS coefficient                                 |
|--------------------|------------------------------------------------|
| Betweenness        | `r round(stability[["betweenness"]], 2)`       |
| Closeness          | `r round(stability[["closeness"]], 2)`         |
| Expected Influence | `r round(stability[["expectedInfluence"]], 2)` |
| Strength           | `r round(stability[["strength"]], 2)`          |

: Correlation Stability (CS) coefficients of the bootstrapped model {#tbl-cscoef}

```{r}
#| label: fig-bootstrapped-model-3
#| fig-cap: "Stability plot"

# Plot stability tests
plot(casedrop, statistics = "all") +
  geom_hline(yintercept = 0.7, color = "grey", linetype = "dashed")
```

@fig-bootstrapped-model-4 shows significance tests of the differences in connection strengths between each node-pair vertex, ordered by connection strength in the original estimated network. In other words, we tested whether two connections significantly differ from each other at $\alpha = 0.05$. Around half of the comparisons are significant. The Work-Illness vertex is significantly different from every other vertex. Traffic-Fire and Hybrid-Polarization are both significantly different from all other vertices except from each other.

```{r}
#| label: fig-bootstrapped-model-4
#| fig-cap: "Connection pair-wise significance tests. Diagonal shows strength of original connection, gray boxes indicate non-significant differences, black boxes indicate significant differences at 0.05."

# Plot bootstrapped significance intervals
plot(
  bootstrap,
  statistics          = "edge",
  plot                = "difference",
  alpha               = 0.05,
  onlyNonZero         = TRUE,
  order               = "mean"
)
```

## Network Comparisons {#sec-networkcomparisons}

```{r}
#| label: network-setup

# Estimate networks for every subgroup separately
net_direct_0 <- data %>%
  dplyr::filter(Direct == 0) %>%
  dplyr::select(Weather:Polarization) %>%
  estimateNetwork(
    default = "EBICglasso",
    corMethod = "cor",
    corArgs = list(method = "spearman"),
    verbose = FALSE,
    nonPositiveDefinite = "continue"
  )
net_direct_1 <- data %>%
  dplyr::filter(Direct == 1) %>%
  dplyr::select(Weather:Polarization) %>%
  estimateNetwork(
    default = "EBICglasso",
    corMethod = "cor",
    corArgs = list(method = "spearman"),
    verbose = FALSE,
    nonPositiveDefinite = "continue"
  )

net_nearmiss_0 <- data %>%
  dplyr::filter(Near.Miss == 0) %>%
  dplyr::select(Weather:Polarization) %>%
  estimateNetwork(
    default = "EBICglasso",
    corMethod = "cor",
    corArgs = list(method = "spearman"),
    verbose = FALSE,
    nonPositiveDefinite = "continue"
  )
net_nearmiss_1 <- data %>%
  dplyr::filter(Near.Miss == 1) %>%
  dplyr::select(Weather:Polarization) %>%
  estimateNetwork(
    default = "EBICglasso",
    corMethod = "cor",
    corArgs = list(method = "spearman"),
    verbose = FALSE,
    nonPositiveDefinite = "continue"
  )
```

We compared networks conditioned on the two control variables (see @sec-controlvariables): Direct experience of fire and near miss-experience of fire. On the system level, we expected a sparser network among those respondents who had experienced a fire, and a denser network among those who had experienced a near-miss event. On the node level, we expected a decrease in the centrality of the Fire node among the direct experience group, and an increase in Fire centrality among the near-miss experience group.

The statistics are shown in @tbl-networkcomps. The networks are graphed in @fig-comp-graphs, where each row is a comparison, the first two columns show the compared groups, and the third column shows the difference between the groups. Note, that no node connection changes have been significance-tested.

```{r}
#| label: networkcomps

# Compare strength between networks
# NB: Using pull request #32 of NCT for multi-core support
comp_direct <- NCT(
  net_direct_0,
  net_direct_1,
  gamma = 0.5,
  it = 1000,
  abs = TRUE,
  test.centrality = TRUE,
  centrality = c("closeness", "betweenness", "strength", "expectedInfluence"),
  nodes = "Fire",
  progressbar = FALSE,
  verbose = FALSE,
  nCores = 16,
  make.positive.definite = TRUE
)

comp_nearmiss <- NCT(
  net_nearmiss_0,
  net_nearmiss_1,
  gamma = 0.5,
  it = 1000,
  abs = TRUE,
  test.centrality = TRUE,
  centrality = c("closeness", "betweenness", "strength", "expectedInfluence"),
  nodes = "Fire",
  progressbar = FALSE,
  verbose = FALSE,
  nCores = 16,
  make.positive.definite = TRUE
)
```

::: {#tbl-networkcomps layout-nrow="2"}
| Test               | Statistic                                                      | Significance                                      |
|------------------|------------------------------|------------------------|
| Network variance   | $\Delta_{O} =$ `r round(comp_direct$nwinv.real, 2)`            | $p =$ `r round(comp_direct$nwinv.pval, 4)`        |
| Global strength    | $\Delta_{S}^{g} =$ `r round(comp_direct$glstrinv.real, 2)`     | $p =$ `r round(comp_direct$glstrinv.pval, 4)`     |
| Closeness          | $\Delta_{C} =$ `r round(comp_direct$diffcen.real[[1]], 2)`     | $p =$ `r round(comp_direct$diffcen.pval[[1]], 4)` |
| Betweenness        | $\Delta_{B} =$ `r round(comp_direct$diffcen.real[[2]], 2)`     | $p =$ `r round(comp_direct$diffcen.pval[[2]], 4)` |
| Strength           | $\Delta_{S}^{l} =$ `r round(comp_direct$diffcen.real[[3]], 2)` | $p =$ `r round(comp_direct$diffcen.pval[[3]], 4)` |
| Expected Influence | $\Delta_{EI} =$ `r round(comp_direct$diffcen.real[[4]], 2)`    | $p =$ `r round(comp_direct$diffcen.pval[[4]], 4)` |

: Direct Experience of Fire (`r length(comp_direct$glstrinv.perm)` permutations) {#tbl-comp-1}

| Test               | Statistic                                                        | Significance                                        |
|------------------|------------------------------|------------------------|
| Network variance   | $\Delta_{O} =$ `r round(comp_nearmiss$nwinv.real, 2)`            | $p =$ `r round(comp_nearmiss$nwinv.pval, 4)`        |
| Global strength    | $\Delta_{S}^{g} =$ `r round(comp_nearmiss$glstrinv.real, 2)`     | $p =$ `r round(comp_nearmiss$glstrinv.pval, 4)`     |
| Closeness          | $\Delta_{C} =$ `r round(comp_nearmiss$diffcen.real[[1]], 2)`     | $p =$ `r round(comp_nearmiss$diffcen.pval[[1]], 4)` |
| Betweenness        | $\Delta_{B} =$ `r round(comp_nearmiss$diffcen.real[[2]], 2)`     | $p =$ `r round(comp_nearmiss$diffcen.pval[[2]], 4)` |
| Strength           | $\Delta_{S}^{l} =$ `r round(comp_nearmiss$diffcen.real[[3]], 2)` | $p =$ `r round(comp_nearmiss$diffcen.pval[[3]], 4)` |
| Expected Influence | $\Delta_{EI} =$ `r round(comp_nearmiss$diffcen.real[[4]], 2)`    | $p =$ `r round(comp_nearmiss$diffcen.pval[[4]], 4)` |

: Near Miss Experience of Fire (`r length(comp_nearmiss$glstrinv.perm)` permutations) {#tbl-comp-2}

Network NCT results by substantive control variable
:::

```{r}
#| label: fig-comp-graphs
#| fig-cap: "Gaussian graphical models of risk perception by substantive control variables, compared using network comparison testing"
#| fig-subcap:
#| -  "No direct experience"
#| -  "Direct experience"
#| -  "Change with direct experience"
#| -  "No near-miss experience"
#| -  "Near-miss experience"
#| -  "Change with near-miss experience"
#| layout: "[[1,1,1],[1,1,1]]"
#| column: screen-inset-shaded

qgraph(net_direct_0$graph,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")
text(-1, -1, paste0("N = ",net_direct_0$nPerson))

qgraph(net_direct_1$graph,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")
text(-1, -1, paste0("N = ",net_direct_1$nPerson))

net_direct_change <- net_direct_1$graph - net_direct_0$graph
qgraph(net_direct_change,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")

qgraph(net_nearmiss_0$graph,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")
text(-1, -1, paste0("N = ",net_nearmiss_0$nPerson))

qgraph(net_nearmiss_1$graph,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")
text(-1, -1, paste0("N = ",net_nearmiss_1$nPerson))

net_nearmiss_change <- net_nearmiss_1$graph - net_nearmiss_0$graph
qgraph(net_nearmiss_change,
       layout = mds_spline$conf,
       edge.labels = TRUE,
       maximum = 0.5,
       details = TRUE,
       theme = "colorblind")

```

Direct experience of fire is related to a weak effect on network structure (@tbl-comp-1). Global strength is significantly different, indicating that the overall connectivity has changed between networks. However, network variance is not significantly different. Since the network is dense and non-smallworld, the difference between variance and strength may be due to small changes in many network edges leading to a overall significant change in the sum of edge strength, while leaving individual maximum changes small [see @vanborkulo2022].

Near-miss experiences have a stronger effect on the network structure (@tbl-comp-2). Network variance and global strength are both significantly different, suggesting that a change in overall network structure has occurred. In both cases, the statistic has grown in the network estimated from respondents that have experienced a near miss. In other words: the network seems to be slightly denser in people who have nearly experienced a fire.

For the Fire node, no changes seem to have occurred in the near-miss comparison. However, for individuals who have a direct fire experience, Expected Influence has dropped significantly. This suggests that the connections between Fire and other nodes has grown slightly more negative. Because the network does not show any negative edges (@fig-comp-graphs-2), this change corresponds to a drop in intensity of positive connections. The Fire node also shows marginally significant changes on closeness and strength in the direct experience comparison. The change in strength mirrors that of the EI change, further suggesting that the centrality of Fire has slightly dropped.

# Discussion

We found the following results for our hypotheses (see @sec-hypotheses):

-  System level:
    -  $H_1^S:$ No support
    -  $H_2^S:$ Strong support
-  Node level:
    -  $H_1^N:$ Partial support
    -  $H_2^N:$ No support

On the system level, we found no support for the hypothesis that a direct experience leads to a sparser risk perception network. Contrary to this, we even found suggestive evidence that a direct experience leads to a *denser* network, in terms of an increase in global strength. However, we found strong support for the hypothesis that an imagined experience leads to a denser network, in terms of increases in both network variance and global strength.

On the node level, the view is different. Here, we found partial support for the hypothesis that a direct experience leads to a decrease in centrality for the hazard event in question (here, the probability of experiencing a fire), but no support for the hypothesis that an imagined experience leads to an increase in the centrality of the hazard event.

For direct experiences, we thus found that network connections grew slightly stronger overall, but that changes were small across the network. Thus, maximum network variance was not significantly related to a direct experience. In our theoretical frame, this suggests that direct experiences may have omnibus effects that are less attributable to single nodes. At the same time, the centrality of the Fire node was shown to slightly decrease, significantly in expected influence and marginally significantly in strength. This suggests that the experience could be connected to a decrease in relevance of the related risk perception for the overall network, specifically in lowering its connective strength with other nodes.

Near-miss experiences seem to be related to significant change on the network level, but no change on the node level. The network grew denser, both in terms of maximum vertex variance and overall global strength, but the Fire node did not significantly change centrality on any measure.

Previous research has shown divergent results when comparing direct experiences with near-misses: direct experiences have been shown to increase perception of risk, while near-misses have been shown to conversely decrease perception. Our results show a slightly more detailed picture. We find that the directness of the experience might have positive effects on *overall* perception but negative effects on *specific* perception. In other words: the affective system grows slightly stronger upon a hazard experience, but the analytic system takes more control over the individual's estimation of the hazard itself.

Near-misses, on the other hand, seem to have a distinct effect on the affective system as a whole, but not on the hazard estimation's relevance within said system. This could be explained with the difference in knowledge: because a direct experience gives knowledge about the hazard in question, it weakens the affective system on its part, while a near-miss does not. Noteworthy, however, is that both experiences still seem to affect the system as a whole, suggesting that the affective system still activates, regardless of the intimacy of the experience.

# References {#sec-references}

::: refs
:::
